---
title: "Analysis of the Value of a College Education"
author: "Aaron Oustrich, Josh Bergstrom, Anna Wolford"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(knitr)
library(ggplot2)
library(GGally)
library(car)
library(MASS)
library(lmtest)
library(vroom)

## Read in data
sal <- vroom("Salary.csv")
sal$MajorCategory <- as.factor(sal$MajorCategory)
sal$Gen <- as.factor(sal$Gen)
```

## 1. 

```{r}
print("Salary Statistics")
summary(sal$Salary)
print("GPA Statistics")
summary(sal$GPA)
print("How many Genders in each Major Category?")
table(sal$MajorCategory,sal$Gen)
```
Above are summary outputs of the continuous variables (Salary and GPA). We show the MajorCategory as a table which shows the number of each Gender in that Major. 

```{r echo=FALSE}
ggplot(sal, aes(x = GPA, y = Salary, color = Gen)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x, aes(group = Gen)) +
  labs(title = "Scatterplot of GPA and Salary",
       x = "GPA",
       y = "Salary") +  
  theme_minimal()
```

This scatterplot shows a positive relationship between GPA and Salary for both genders. Based on the lines of best fit, Male salary appears higher than Female salary for all GPA levels. It's interesting to note that both genders appear to have similar slope which leads us to believe the effect of GPA is consistent across the genders.

```{r echo=FALSE}
ggplot(sal, aes(x = MajorCategory, y = Salary, fill = MajorCategory)) +
  geom_boxplot() +
  labs(title = "Box Plot of Salary by Major Category",
       x = "Major Category",
       y = "Salary") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```


```{r echo=FALSE}
ggplot(sal, aes(x = MajorCategory, fill = Gen)) +
  geom_bar(position = "dodge") +
  labs(title = "Bar Graph of Major Category Counts by Gender",
       x = "Major Category",
       y = "Count",
       fill = "Gender") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  
```


These last two plots show there are few outliers in salary across all the major categories and that there are more women represented in each major category than men (though they are all relatively equal).


## 2.

The multiple linear regression model is based on the following equation:

$$\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}$$
Where:

- $\mathbf{y}$ is the vector of salaries

- $\mathbf{X}$ is the matrix of explanatory variables which includes a column of 1s for the intercept and dummy-encoded columns for the categorical variables of Major Category and Gender.

- $\mathbf{\beta}$ is the vector of coefficients

- $\mathbf{\epsilon}$ is the vector of errors which are assumed to be normally distributed with mean 0 and constant variance $\sigma^2$ ( $\mathbf{\epsilon} \sim N(\mathbf{0}, \sigma^2\mathbf{I})$)


Thus, the model is specified as: 
$$\mathbf{y} \sim \text{MVN}(\mathbf{X\beta},\sigma^2\mathbf{I})$$

If we fit the data to the specified model, and confirm the model assumptions are sufficiently met, we can determine the effect of major choice and identify any gender discrimination.

## 3. 

```{r}
sal.lm <- lm(Salary ~., data=sal)
summary(sal.lm)

X <- model.matrix(Salary ~., data=sal)
y <- sal$Salary
bhat <- solve(t(X)%*%X)%*%t(X)%*%y
bhat_table <- as.table(bhat)
colnames(bhat_table) <- "Beta Estimates"
bhat_table

# Estimate of residual variance
s2 <- t(y-X %*% bhat)%*% (y-X%*%bhat) /(nrow(sal)-ncol(X))
s2

sqrt(s2)
sigma(sal.lm)

# R^2
summary(sal.lm)$r.squared
```

Compared to women, on average men make $5931.63 more, holding all else constant. 
With every unit increase in GPA, holding all else constant, the average salary increases by $5,488.74.

## 4. 
```{r echo=FALSE}
ggplot(sal, aes(x = MajorCategory, y = Salary, fill = Gen)) +
  geom_boxplot(position = "dodge") +
  labs(title = "Box Plot of Salary by Major Category and Gender",
       x = "Major Category",
       y = "Salary",
       fill = "Gender") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

``` {r}
sal.full <- lm(Salary ~ MajorCategory + Gen + GPA + MajorCategory:Gen, data = sal)

anova(sal.full, sal.lm)
```

To see if there is an interaction between MajorCategory and Gender, we created a new model (`sal.full`) which adds interaction terms for these variables of interest. The null hypothesis is that there is no interaction between MajorCategory and Gender (i.e. all the $\beta$ coefficients for the interaction terms are all 0). The alternative hypothesis is that there is a significant interaction between MajorCategory and Gender.

We evaluated this hypothesis using an ANOVA test on the two models and got an F statistic of 4.3 with a p-value less than 0.05. Therefore, we reject the null hypothesis and conclude that there is a significant interaction between MajorCategory and Gender.

It appears that men have a higher salary across all majors. Personally, we don't feel that we can comment on "gender discrimination" even if there is an apparent difference, because we don't know enough about the data (ex: how these salaries were negotiated or how salaries may be different in the same company). 


## 5.

### Linearity Assumption

<<<<<<< HEAD
**5. The validity of the tests from #4 depend on the validity of the assumptions in your model (if your assumptions are violated then the values are likely wrong). Create graphics and/or run appropriate hypothesis tests to check the L-I-N-E assumptions associated with your multiple linear regression model including any interactions you found in #4. State why each assumption does or does not hold for the salary data.**
<!-- DISCUSS: 'including any interactions you found in #4 -->

We begin by testing for linearity. We do so by running an AVplot. 
```{r}
car::avPlots(sal.lm)
```
Because the only continuous variable GPA does not appear to be not linear, I believe my linearity assumption is met.

For testing for normality, we consult a histogram of our standardized residuals. <!-- We can also check the ks.test - should I write out that name? -->
```{r}
ggplot() +
  geom_histogram(mapping = aes(x = MASS::stdres(sal.lm)), bins = 30, fill = "blue", color = "black") +
  labs(title = "Histogram of Standardized Residuals",
       x = "Standardized Residuals",
       y = "Frequency")

ks.test(MASS::stdres(sal.lm), "pnorm")
```
The histogram of standardized residuals appears to be normally distributed. 

Given p value of .401, we do not have sufficient evidence to reject the null hypothesis that the standardized residuals deviate significantly from a normal distribution.

```{r}
sal.lm
```



**6. Calculate 97% confidence intervals for the coefficients for GPA, Gender and one major category. Interpret each interval.**


**7. For the Computers and Mathematics major category, perform a general linear hypothesis test that women, on average, earn less salary than men (for the same GPA). State your hypotheses, -value and conclusion. If this test is significant, report and estimate a 95% confidence interval for how much more men earn than women in that major category.**

<!-- is this actually only looking at the computers and mathematics major category? why does it equal the coeff GenM on summary(sal.lm)? -->
=======
>>>>>>> e3ec90b (Finished)
```{r}
avPlots(sal.full)
```

Based on all the added variable plots above, we believe the Linearity assumption is met. Looking at the added variable plot of the only continuous variable (GPA) is linear. All other categorical plots look weird, but there's nothing in them that suggest a non-linear relationship.

### Independence Assumption

We don't know exactly how the data was collected, so we can't say for sure if the data were randomly sampled and independent of one another. However, we know that a salary for one person doesn't typically depend on the salary of another person, so we can assume independence. This assumption is met, and we can proceed with the analysis by checking the other assumptions.

### Normality Assumption
```{r echo=FALSE}
std_resids <- MASS::stdres(sal.full)

ggplot() +
  geom_histogram(mapping = aes(x = std_resids), bins = 30, fill = "blue", color = "black") +
  labs(title = "Histogram of Standardized Residuals",
       x = "Standardized Residuals",
       y = "Frequency")
```

``` {r}
ks.test(std_resids, "pnorm")
```

Because the histogram of the standardized residuals looks roughly normal and the p-value of the Kolmogorov-Smirnov test is greater than 0.05, we fail to reject the null hypothesis and conclude that the standardized residuals are normally distributed. Therefore, we will say this assumption is met.

### Equal Variance Assumption
```{r message=FALSE, echo=FALSE}
sal %>% 
  ggplot(aes(x = sal.full$fitted.values,
             y = std_resids)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = F) +
  labs(x = "Fitted Values",
       y = "Standardized Residuals")
```

``` {r}
lmtest::bptest(sal.full)
```

We can see from the fitted values vs. standardized residuals scatter plot that the spread of the points at any value along the x-axis is roughly the same. Also, the Breusch-Pagan test produces a large p-value. This means we fail to reject the null hypothesis and have insufficient information to say that the variance of the data is unequal. Therefore, we will say this assumption is met.

## 6.
```{r}
confint(sal.full,"GPA", level = 0.97)
confint(sal.full, "GenM", level = 0.97)
confint(sal.full, "MajorCategoryArts", level = 0.97)
```

The 97% confidence interval for the coefficient of GPA is (4646.385, 6129.755). This means that we are 97% confident that for every unit increase in GPA, while holding all other variables constant, average salary increases between \$4,646.39 and \$6,129.76.

The 97% confidence interval for the coefficient of GenM is (939.567, 24387.63). This means that we are 97% confident that a male's salary on average is between \$4,646.39 and \$6,129.76 higher than a female's while holding all other variables constant.

The 97% confidence interval for the coefficient of MajorCategoryArts is (-3377.766, 4805.189). This means that we are 97% confident that the average salary for the Arts major category is between \$3,377.77 lower and \$4,805.19 higher than the average salary for the baseline major (Agriculture and Natural Resources), while holding all else constant.

## 7. 
```{r}
a1 <- c(1, #intercept
        0,0,0,0, 
        1, #computer and math
        0,0,0,0,0,0,0,0,0,0,
        1, #genM
        2.5, #gpa
        0,0,0,0,
        1, # math interaction man
        0,0,0,0,0,0,0,0,0,0)

a2 <- c(1, #intercept
        0,0,0,0, 
        1,#computer and math
        0,0,0,0,0,0,0,0,0,0,
        0, #genF
        2.5, #gpa
        0,0,0,0,
        0, # math interaction man
        0,0,0,0,0,0,0,0,0,0)

my.test <- multcomp::glht(sal.full, linfct=t(a1-a2), alternative="two.sided")
summary(my.test)
confint(my.test, level = 0.95)
```

H0: Women's salary for computer and math = Men's salary for computer and math
HA: Women's salary for computer and math != Men's salary for computer and math

Based on the results of our general linear hypothesis test, we have a p-value of 1.53e-05 so we reject the null hypothesis and conclude that women's salary for computer and math not equal to men's salary for computer and math major. The 95% confidence interval for the difference in average salary between men and women in the computer and math major category is (4339.6592,11468.8276). This means that we are 95% confident that the average salary of a man who majored in computer and math is between \$4,339.66 and \$11,468.83 higher than the average salary for women of the same major, while holding all else constant.

## 8.

```{r}
new.josh <- data.frame(MajorCategory='Computers & Mathematics', GPA = 3.98, Gen="M")
predict.lm(sal.full, new.josh, interval="prediction", level=0.95)

new.aaron <- data.frame(MajorCategory='Computers & Mathematics', GPA = 3.81, Gen="M")
predict.lm(sal.full, new.aaron, interval="prediction", level=0.95)

new.anna <- data.frame(MajorCategory='Computers & Mathematics', GPA = 3.5, Gen="F")
predict.lm(sal.full, new.anna, interval="prediction", level=0.95)
```

For Josh, who is a male computers & mathematics major with a 3.98 GPA we are 95% confident that his average salary lies somewhere between \$82,932.69 and \$104,253.80.

For Aaron, who is a male computers & mathematics major with a 3.81 GPA we are 95% confident that his average salary lies somewhere between \$82,019.24 and \$103,335.30.

For Anna, who is a female computers & mathematics major with a 3.5 GPA we are 95% confident that her average salary lies somewhere between \$72,612.75 and \$93,592.68.

## 9. 
```{r}
n <- nrow(sal)
rpmse <- rep(x=NA, times=n)
wid <- rep(x=NA, times=n)



for(i in 1:n){
  ## Select test observations

  
  ## Split into test and training sets
  test.set <- sal[i,]
  train.set <- sal[-i,]
  
  ## Fit a lm() using the training data
  train.lm <- lm(Salary ~.+MajorCategory:Gen, data=train.set)
  
  ## Generate predictions for the test set
  my.preds <- predict.lm(train.lm, newdata=test.set, interval="prediction")
  
  
  ## Calculate RPMSE
  rpmse[i] <- (test.set[['Salary']]-my.preds[,'fit'])^2 %>% mean() %>% sqrt()
  
  
  ## Calculate Width
  wid[i] <- (my.preds[,'upr'] - my.preds[,'lwr']) %>% mean()
  
}

# RPMSE
hist(rpmse, main="RPMSE Histogram", xlab="RPMSE")
mean(rpmse) #rpmse is how off you are on average 

#standard deviation of salary = 10996.17

(var(sal$Salary) - mean(rpmse)^2 ) / var(sal$Salary)  # 84.29% of overall variance reduction

# Width histogram
hist(wid, main="Width Histogram", xlab="Width")
mean(wid)
```

Using the average RPMSE from our Leave One Out Cross-validation and the variance of the salary in the dataset, we calculated an overall variance reduction of 84.29%. 

The average width of the predicted intervals is \$21,013.43. This means that on average, the predictions are about \$10,506.72 over or under the actual salary.

Both the width and RPMSE histograms above are right-skewed so the average values we report above are made higher than most of the cross-validated values. Overall, we think our model is fairly accurate at predicting new salaries. 
